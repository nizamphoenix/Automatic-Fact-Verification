{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0527 01:56:32.544157 140736874374080 file_utils.py:35] Deprecated cache directory found (/Users/linzhijia/.allennlp/datasets).  Please remove this directory from your system to free up space.\n",
      "/Users/linzhijia/Library/Python/3.6/lib/python/site-packages/sklearn/utils/linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Text entailment predictor\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/esim-elmo-2018.05.17.tar.gz\", 'textual-entailment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nizam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_md'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-eee36785e0e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'``'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'--'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-lrb-\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-rrb-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_md'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/nizam/.local/lib/python3.6/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nizam/.local/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_md'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from math import log, sqrt\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "# Stopwords\n",
    "stop = stopwords.words('english') + list(string.punctuation)+list(('``','--'))+list((\"-lrb-\", \"-rrb-\"))\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wiki-pages-text/wiki-001.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2d4bad14fca8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#term_id = -1 # global variable for assigning unique id to word types(vocabulary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#looping 109 files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwikifile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#opening 1 file at a time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mfile_as_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwikifile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#reading 1 file into a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_as_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wiki-pages-text/wiki-001.txt'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "filenames = []\n",
    "for i in range(1,10):\n",
    "    filenames.append('wiki-pages-text/wiki-00'+ str(i) + '.txt')\n",
    "for i in range(10,100):\n",
    "    filenames.append('wiki-pages-text/wiki-0'+ str(i)+'.txt')\n",
    "for i in range(100,110):\n",
    "    filenames.append('wiki-pages-text/wiki-'+ str(i)+'.txt')\n",
    "\n",
    "file_as_list = []#contains a file read from wiki-pages-text\n",
    "#wiki_sentences = []\n",
    "all_wiki_sentences = []\n",
    "identifier = []\n",
    "st = time.time()\n",
    "files_processed = 0\n",
    "#term_id = -1 # global variable for assigning unique id to word types(vocabulary)\n",
    "for filename in filenames:#looping 109 files\n",
    "    with open(filename) as wikifile:#opening 1 file at a time\n",
    "        file_as_list.append(wikifile.readlines())#reading 1 file into a list\n",
    "    for file in file_as_list:\n",
    "        for sent in file:\n",
    "            page_id,sent_num,sent = sent.split(\" \",2)\n",
    "            #wiki_sentences.append(sent)\n",
    "            all_wiki_sentences.append(sent)\n",
    "            identifier.append((page_id,sent_num))\n",
    "    #write_to_file(processed_sents,uid)\n",
    "    #write_to_file(wiki_sentences)\n",
    "    file_as_list = []\n",
    "    #wiki_sentences = []\n",
    "    files_processed += 1\n",
    "et = time.time()\n",
    "print(\"files_processed: \", files_processed)\n",
    "print(\"Time for processing wiki-corpus:\",(et-st)/(3600.0),\"hrs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()  \n",
    "processed_sents = []\n",
    "# vocab contains (term, term id) pairs\n",
    "#vocab = {}\n",
    "# total_tokens stores the total number of tokens\n",
    "total_tokens = 0\n",
    "# total_stems stores the total number of terms in vocab\n",
    "#total_lemmas = 0\n",
    "\n",
    "for sent in all_wiki_sentences:\n",
    "    norm_sent = preprocessed_sentence(sent)\n",
    "    for token in norm_sent:\n",
    "        total_tokens += 1\n",
    "        #if token not in vocab:\n",
    "            #vocab[token] = total_lemmas\n",
    "            #total_lemmas += 1\n",
    "    processed_sents.append(norm_sent)  \n",
    "et = time.time()   \n",
    "print(\"Time for processing wiki-corpus:\",(et-st)/(3600.0),\"hrs.\")\n",
    "print(\"Number of documents = {}\".format(len(processed_sents)))\n",
    "#print(\"Number of unique terms = {}\".format(len(vocab)))\n",
    "print(\"Number of tokens = {}\".format(total_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_term_freqs stores the counters (mapping terms to term frequencies) of all documents\n",
    "doc_term_freqs = []\n",
    "st = time.time()\n",
    "for tokens in processed_sents:\n",
    "    tfs = Counter()\n",
    "    for token in tokens:\n",
    "        tfs[token] += 1\n",
    "    doc_term_freqs.append(tfs)\n",
    "et = time.time()\n",
    "print(\"Time used: \",(et - st)/60.0,\" mins\") \n",
    "print(len(doc_term_freqs))\n",
    "print(doc_term_freqs[0])\n",
    "print(doc_term_freqs[-109])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "    def __init__(self, vocab, doc_term_freqs):\n",
    "        self.vocab = vocab\n",
    "        self.doc_len = [0] * len(doc_term_freqs)\n",
    "        self.doc_term_freqs = [[] for i in range(len(vocab))]\n",
    "        self.doc_ids = [[] for i in range(len(vocab))]\n",
    "        self.doc_freqs = [0] * len(vocab)\n",
    "        self.total_num_docs = 0\n",
    "        self.max_doc_len = 0\n",
    "        for docid, term_freqs in enumerate(doc_term_freqs):\n",
    "            doc_len = sum(term_freqs.values())\n",
    "            self.max_doc_len = max(doc_len, self.max_doc_len)\n",
    "            self.doc_len[docid] = doc_len\n",
    "            self.total_num_docs += 1\n",
    "            for term, freq in term_freqs.items():\n",
    "                try:\n",
    "                    term_id = vocab[term]\n",
    "                    self.doc_ids[term_id].append(docid)\n",
    "                    self.doc_term_freqs[term_id].append(freq)\n",
    "                    self.doc_freqs[term_id] += 1\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "    def num_terms(self):\n",
    "        return len(self.doc_ids)\n",
    "\n",
    "    def num_docs(self):\n",
    "        return self.total_num_docs\n",
    "\n",
    "    def docids(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_ids[term_id]\n",
    "\n",
    "    def freqs(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_term_freqs[term_id]\n",
    "\n",
    "    def f_t(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_freqs[term_id]\n",
    "\n",
    "    def space_in_bytes(self):\n",
    "        # this function assumes each integer is stored using 8 bytes\n",
    "        space_usage = 0\n",
    "        for doc_list in self.doc_ids:\n",
    "            space_usage += len(doc_list) * 8\n",
    "        for freq_list in self.doc_term_freqs:\n",
    "            space_usage += len(freq_list) * 8\n",
    "        return space_usage\n",
    "    \n",
    "invindex = InvertedIndex(vocab, doc_term_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_object(obj, filepath):\n",
    "    '''\n",
    "    function to save a pickle object\n",
    "    '''\n",
    "    with open(filepath, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Save the inverted index for future use\n",
    "save_object(invindex, 'invindex_final.pkl')\n",
    "\n",
    "def load_object(filepath):\n",
    "    '''\n",
    "    function to load a pickle object\n",
    "    '''\n",
    "    obj=None\n",
    "    with (open(filepath, \"rb\")) as openfile:\n",
    "        while True:\n",
    "            try:\n",
    "                obj = pickle.load(openfile)\n",
    "            except EOFError:\n",
    "                break\n",
    "    return obj\n",
    "\n",
    "# Re-load the index\n",
    "invindex=load_object('invindex_final.pkl')               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess\n",
    "import misc\n",
    "doc_term_freq,vocab=create_doc_term_freq(filepath)\n",
    "invindex = InvertedIndex(vocab, doc_term_freqs)\n",
    "save_object(invindex, 'invindex_final.pkl')\n",
    "#invindex=load_object('invindex_final.pkl')               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sentences Retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_my_token(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "def preprocessed_sentence(sent):\n",
    "    norm_sent = []#contains normalized tokens of a single sentence(treated as a document)\n",
    "    for token in nltk.word_tokenize(sent.lower()):#1.all tokens lowercased\n",
    "        nfd_token = unicodedata.normalize('NFD',token)#2.NFD normalization applied\n",
    "        if nfd_token not in stop:#3.stop words and punctuations excluded\n",
    "            #norm_token = lemmatize_my_token(nfd_token)#4.lemmatized\n",
    "            #norm_sent.append(norm_token)\n",
    "            norm_sent.append(nfd_token)\n",
    "    return norm_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Using bm25 ro retrieve sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_term_freqs(preprocessed_sent):\n",
    "    tfs = Counter()    \n",
    "    for token in preprocessed_sent:\n",
    "        tfs[token] += 1\n",
    "    return tfs\n",
    "\n",
    "#compute BM25\n",
    "N = invindex.num_docs()\n",
    "Lavg = sum(invindex.doc_len)/N\n",
    "\n",
    "# query: a claim index: the inverted index   k: the maximun number of sentences returned\n",
    "def bm_25(query, index, k):      \n",
    "    \n",
    "    scores_bm25 = Counter()\n",
    "    #scores_tfidf = Counter()\n",
    "    query_terms = preprocessed_sentence(query)\n",
    "    query_terms_freqs = extract_term_freqs(query_terms)\n",
    "    k1 = 1.2\n",
    "    k3 = 1.5\n",
    "    b = 0.75\n",
    "    \n",
    "    def fill_scores_one(term):\n",
    "        try:\n",
    "            f_term = index.f_t(str(term))         #number of docs containing the term \n",
    "            docids = index.docids(str(term))    #list of Ids of the documents containing the term 't'\n",
    "            fqt = query_terms_freqs[str(term)]   # number of term in a query\n",
    "            \n",
    "            \n",
    "            def fill_scores_two(d, docid): \n",
    "           \n",
    "            #for d,docid in enumerate(docids):   \n",
    "            \n",
    "                fdt =  index.freqs(term)[d]  # number of a term in a sentence\n",
    "                Ld = index.doc_len[docid]    # length of a sentence\n",
    "                idf = log((N - f_term + 0.5)/(f_term + 0.5))\n",
    "                tf_doc = ((k1 + 1) * fdt)/(k1 * ((1-b) + b * Ld/Lavg) + fdt)\n",
    "                query_tf = (k3 + 1) * fqt / (k3 + fqt)\n",
    "                wt = idf * tf_doc * query_tf\n",
    "                scores_bm25[docid] += wt\n",
    "            \n",
    "            temp = [*map(fill_scores_two, list(range(len(docids))), docids)]\n",
    "            temp = []\n",
    "             \n",
    "                #temp = 0 #an auxiliary variable to aid in computing the score.\n",
    "                #temp = temp + log(1 + fdt) * log(N/float(f_term))\n",
    "                #scores_tfidf[docid] += 1.0/sqrt(index.doc_len[docid]) * temp\n",
    "        except KeyError:\n",
    "            pass\n",
    "        except IndexError:\n",
    "            print(\"Index error occured. Try again later.\")\n",
    "\n",
    "    \n",
    "    dummy = [*map(fill_scores_one, query_terms)]\n",
    "    dummy = []\n",
    "    \n",
    "    #final_docids = set()\n",
    "    #for x in scores_bm25.most_common(k):\n",
    "        #final_docids.add(x[0])\n",
    "    #for x in scores_tfidf.most_common(k):\n",
    "        #final_docids.add(x[0])\n",
    "    return [x[0] for x in scores_bm25.most_common(k)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Using the entities in the claim to retrive sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of processed identifiers\n",
    "proccessed_ids = defaultdict(list)\n",
    "for doc_id, pair in enumerate(identifier):\n",
    "    proccessed_id = unicodedata.normalize('NFD',re.sub('_',' ', re.sub('-(\\w)+-','', pair[0])).lower())\n",
    "    proccessed_id[proccessed_id].append((pair[1], doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types of the entities in the claim\n",
    "type_words = ['film', 'movie', 'song', 'band', 'television series', 'novel', \\\n",
    "              'comics', 'magazine', 'play', 'soundtrack']\n",
    "\n",
    "def find_ents_ids(query):\n",
    "    \n",
    "    query = unicodedata.normalize('NFD',query)\n",
    "    evidence_ids = []\n",
    "    \n",
    "    # Search possible type words\n",
    "    query_type = None\n",
    "    for type_word in type_words:\n",
    "        if type_word in query.lower():\n",
    "            if type_word == 'movie':\n",
    "                query_type = 'film'\n",
    "                break\n",
    "            if type_word == 'television series':\n",
    "                query_type = 'tv series'\n",
    "                break\n",
    "            query_type = type_word\n",
    "            break   \n",
    "    \n",
    "    # Find capitaltized tokens\n",
    "    temp = []\n",
    "    num_cap = 0\n",
    "    cap_token = []\n",
    "    tokens = word_tokenize(query)\n",
    "    length_query = len(tokens)\n",
    "    i = 0\n",
    "    for token in tokens:\n",
    "        i += 1\n",
    "        if token[0].isupper():\n",
    "            num_cap += 1\n",
    "            cap_token.append(token)\n",
    "            if i == length_query:\n",
    "                temp.append(' '.join(cap_token))\n",
    "        else:\n",
    "            if num_cap == 0:\n",
    "                continue\n",
    "            else:\n",
    "                if cap_token[-1] in ['of', 'a', 'the']:\n",
    "                    cap_token.append(token)\n",
    "                elif token in ['of', 'a', 'the'] and token != cap_token[-1]:\n",
    "                    cap_token.append(token)\n",
    "                elif token.isnumeric():\n",
    "                    cap_token.append(token)  \n",
    "                else:\n",
    "                    if num_cap > 0:\n",
    "                        temp.append(' '.join(cap_token))\n",
    "                        num_cap = 0\n",
    "                        cap_token = []\n",
    "\n",
    "    cap_tokens = [re.sub('the ', '', token.lower()) for token in temp]\n",
    "    #print(cap_tokens)\n",
    "    \n",
    "    def find_matching(item):\n",
    "        return list(filter(lambda x:item in x, list(proccessed_ids.keys())))\n",
    "\n",
    "    # Find identifiers that contain the capitaltized tokens\n",
    "    ids_list = [*map(find_matching, cap_tokens)]\n",
    "    i = 0\n",
    "    for ids in ids_list:\n",
    "        if len(ids) <= 3:\n",
    "            evidence_ids.extend(ids)\n",
    "        else:\n",
    "            if query_type != None:\n",
    "                item = cap_tokens[i]+\" \"+query_type\n",
    "                evidence_ids.extend(list(filter(lambda x: item in x, ids)))  \n",
    "            else:\n",
    "                if len(evidence_ids) == 0:\n",
    "                    item = cap_tokens[i]\n",
    "                    evidence_ids.extend(list(filter(lambda x : x == item, ids)))\n",
    "        i += 1\n",
    "    return evidence_ids\n",
    "\n",
    "#  Find all document ids for all identifiers\n",
    "def find_ents_sentences(query):\n",
    "    ids = find_ents_ids(query)\n",
    "    evidence_docids = []\n",
    "    for id in ids:\n",
    "        for _,doc_id in proccessed_ids[id]:\n",
    "            evidence_docids.append(doc_id)\n",
    "    return evidence_docids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the result of the two methods\n",
    "def find_possible_sentences(query, k):    \n",
    "    results_bm25 = bm_25(query, invindex, k)\n",
    "    results_ents = find_ents_sentences(query)\n",
    "    sents = set()\n",
    "    for res in results_bm25:\n",
    "        sents.add((res, identifier[res]))\n",
    "    for res in results_ents:\n",
    "        sents.add((res, identifier[res]))\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1.1 Function for Generating 3 Sentence for a training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Selection of training data set</b>\n",
    "<b>1.</b>Select only those training records where length of evidences is atmost 25.\n",
    "<b>2.</b>Randomly Select 100 if 'SUPPORT' labelled or 200 if 'REFUTE' labelled from each set of records whose number of evidences whose length is between 1 and 25;\n",
    "if the length of evidences is less than 100 then select all.\n",
    "<b>3.</b>Therefore, 100 x 25 = 2500 records from support, refute and random 2500 from not enough case since\n",
    "when dealing with not enough case we do not have any evidences in training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('train.json') as trainfile:\n",
    "    train_data = json.load(trainfile)\n",
    "training_support_data = []\n",
    "training_refute_data = []\n",
    "training_notenough_data = []\n",
    "for record in train_data:\n",
    "    if train_data[record]['label']=='SUPPORTS':\n",
    "        training_support_data.append(train_data[record])\n",
    "    elif train_data[record]['label']=='REFUTES':\n",
    "        training_refute_data.append(train_data[record])\n",
    "    elif train_data[record]['label']=='NOT ENOUGH INFO':\n",
    "        training_notenough_data.append(train_data[record])\n",
    "assert(len(train_data)==len(training_support_data)+len(training_refute_data)+len(training_notenough_data))\n",
    "print(\"--------------------Distribution in training dataset-----------------------------------------\")\n",
    "print('Percent of support entries %.3f %%'%(100*(len(training_support_data)/len(train_data))))\n",
    "print('Percent of refute entries %.3f %%'%(100*(len(training_refute_data)/len(train_data))))\n",
    "print('Percent of not-enough entries %.3f %%'%(100*(len(training_notenough_data)/len(train_data))))\n",
    "print(\"---------------------------------------------------------------------------------------------\")\n",
    "#1.\n",
    "temp=[]\n",
    "temp2={}\n",
    "refute = {}\n",
    "support = {}\n",
    "MAX_LENGTH_OF_EVIDENCES = 25\n",
    "for record in training_refute_data:\n",
    "    temp.append(len(record['evidence']))\n",
    "for i in range(1,MAX_LENGTH_OF_EVIDENCES+1):#Note:the max number of evidences for SUPPORT/REFUTE is 48\n",
    "    temp2[i]=0\n",
    "temp = sorted(temp)\n",
    "temp = temp[0:temp.index(MAX_LENGTH_OF_EVIDENCES+1)]\n",
    "for i in temp:\n",
    "    temp2[i]+=1\n",
    "refute =temp2\n",
    "#refute is a dictionary where key=length of evidences and value=no of evidences whose length is as indicated by key\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "temp=[]\n",
    "temp2={}\n",
    "for record in training_support_data:\n",
    "    temp.append(len(record['evidence']))\n",
    "for i in range(1,MAX_LENGTH_OF_EVIDENCES+1):#Note:the max number of evidences for SUPPORT/REFUTE is 48\n",
    "    temp2[i]=0\n",
    "temp = sorted(temp)\n",
    "temp = temp[0:temp.index(MAX_LENGTH_OF_EVIDENCES+1)]\n",
    "for i in temp:\n",
    "    temp2[i]+=1\n",
    "support=temp2\n",
    "#support is a dictionary where key=length of evidences and value=no of evidences whose length is as indicated by key\n",
    "#2\n",
    "import numpy as np\n",
    "def get_indices_and_shuffle(dataset):\n",
    "    length_indicies = {}#dictionary...key=length of evidences value:list of indices\n",
    "    for i in list(range(1,MAX_LENGTH_OF_EVIDENCES+1)):\n",
    "        temp=list()\n",
    "        length_indicies[i]=temp\n",
    "    for record in dataset:\n",
    "        if len(record['evidence']) in list(range(1,26)):\n",
    "            temp_list = length_indicies[len(record['evidence'])]\n",
    "            temp_list.append(dataset.index(record))\n",
    "    return length_indicies\n",
    "length_indicies_support = get_indices_and_shuffle(training_support_data)\n",
    "length_indicies_refute = get_indices_and_shuffle(training_refute_data)\n",
    "sample_not_enough = random.sample(training_notenough_data,2500)\n",
    "import random\n",
    "def shuffle_indices(length_indicies_dict,k):\n",
    "    final_indices = []\n",
    "    for length,index_list in length_indicies_dict.items():\n",
    "        if(len(index_list)<=k):\n",
    "            final_indices.extend(index_list)\n",
    "        else:\n",
    "            final_indices.extend(random.sample(index_list, k))\n",
    "    if(len(final_indices)<2500):\n",
    "        final_indices.extend(random.sample(length_indicies_dict[3],2500-len(final_indices)))\n",
    "        final_indices = set(final_indices)\n",
    "    np.random.shuffle(list(final_indices))\n",
    "    return final_indices\n",
    "refute_indices  = shuffle_indices(length_indicies_refute,200)\n",
    "support_indices = shuffle_indices(length_indicies_support,100)\n",
    "training_sample = []\n",
    "for index in refute_indices:\n",
    "    training_sample.append(training_refute_data[index])\n",
    "for index in support_indices:\n",
    "    training_sample.append(training_support_data[index])\n",
    "training_sample.extend(sample_not_enough)\n",
    "np.random.shuffle(training_sample)\n",
    "assert(len(training_sample)==len(refute_indices)+len(support_indices)+len(sample_not_enough))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evidence_docid(evidence):\n",
    "    evidence_id = unicodedata.normalize('NFD',re.sub('_',' ', re.sub('-(\\w)+-','', evidence[0])).lower())\n",
    "    evidence_sent_num = str(evidence[1])\n",
    "    for sent_num, docid in proccessed_ids[evidence_id]:\n",
    "        if evidence_sent_num == sent_num:\n",
    "            return docid\n",
    "            break\n",
    "\n",
    "def get_evidence_output(docid, query):\n",
    "    \n",
    "    norm_id = unicodedata.normalize('NFD',re.sub('_',' ', re.sub('-LRB-(\\w)+-RRB-','', identifier[docid][0])))\n",
    "    sent = clean_sent(all_wiki_sentences[docid])\n",
    "    for word in pronouns_one:\n",
    "        if word in sent:\n",
    "            sent = re.sub(word, ' '+ norm_id + ' ', sent)\n",
    "    for word in pronouns_two:\n",
    "        if word in sent:\n",
    "            sent = re.sub(word, ' ' + norm_id + '\\'s ', sent)    \n",
    "    sent = re.sub('.\\n','',sent)\n",
    "    #print(sent)\n",
    "    \n",
    "    sent_embedding = session.run(embedded_text, feed_dict={text_input: [sent]})\n",
    "    query_embedding = session.run(embedded_text, feed_dict={text_input: [query]})\n",
    "    sim = list(cosine_similarity(sent_embedding, query_embedding).flatten())\n",
    "    score = get_label_scores(query, sent)\n",
    "    #print(score)\n",
    "    return (sim[0], score[0], score[1], score[2])\n",
    "\n",
    "N = invindex.num_docs()\n",
    "def get_training_data_for_sample(sample):\n",
    "    LIMIT = 3\n",
    "    if sample['label'] == 'NOT ENOUGH INFO':\n",
    "        output = label_and_evidenve(sample['claim'] , LIMIT)\n",
    "    else:\n",
    "        docids = [get_evidence_docid(evidence) for evidence in sample['evidence'][:LIMIT]]\n",
    "        if len(docids) == 3:\n",
    "            output = [*map(lambda docid:(get_evidence_output(docid, sample['claim'])), docids)]\n",
    "        else:\n",
    "            output_a = [*map(lambda docid:(get_evidence_output(docid, sample['claim'])), docids)]\n",
    "            if len(docids) == 1:\n",
    "                if docids[0] + 2 <= N - 1:\n",
    "                    output_b = [*map(lambda k:(get_evidence_output(docids[0] + k, sample['claim'])), list(range(1,3)))]\n",
    "                    output = output_a + output_b \n",
    "                else:\n",
    "                    output_b = [*map(lambda k:(get_evidence_output(docids[0] - k, sample['claim'])), list(range(1,3)))]\n",
    "                    output = output_a + output_b \n",
    "            if len(docids) == 2:\n",
    "                if docids[0] + 1 <= N - 1:\n",
    "                    output_b = [get_evidence_output(docids[0] + 1, sample['claim'])]\n",
    "                    output = output_a + output_b\n",
    "                else:\n",
    "                    output_b = [get_evidence_output(docids[0] - 1, sample['claim'])]\n",
    "                    output = output_a + output_b\n",
    "\n",
    "#             if len(docids) == 3:\n",
    "#                 output_b = [*map(lambda docid:(get_evidence_output(docid + 1, sample['claim'])), docids)]\n",
    "#             if len(docids) == 4:\n",
    "#                 output_b = [get_evidence_output(docids[0] + 1, sample['claim'])]\n",
    "#             output = output_a + output_b \n",
    "                \n",
    "    return output[:LIMIT]                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 3.1.2 Function for Generating 4 Sentence for a training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evidence_docid(evidence):\n",
    "    evidence_id = unicodedata.normalize('NFD',re.sub('_',' ', re.sub('-(\\w)+-','', evidence[0])).lower())\n",
    "    evidence_sent_num = str(evidence[1])\n",
    "    for sent_num, docid in proccessed_ids[evidence_id]:\n",
    "        if evidence_sent_num == sent_num:\n",
    "            return docid\n",
    "            break\n",
    "\n",
    "# Generating \n",
    "def get_evidence_output(docid, query):\n",
    "    \n",
    "    norm_id = unicodedata.normalize('NFD',re.sub('_',' ', re.sub('-LRB-(\\w)+-RRB-','', identifier[docid][0])))\n",
    "    sent = clean_sent(all_wiki_sentences[docid])\n",
    "    for word in pronouns_one:\n",
    "        if word in sent:\n",
    "            sent = re.sub(word, ' '+ norm_id + ' ', sent)\n",
    "    for word in pronouns_two:\n",
    "        if word in sent:\n",
    "            sent = re.sub(word, ' ' + norm_id + '\\'s ', sent)    \n",
    "    sent = re.sub('.\\n','',sent)\n",
    "    #print(sent)\n",
    "    \n",
    "    sent_embedding = session.run(embedded_text, feed_dict={text_input: [sent]})\n",
    "    query_embedding = session.run(embedded_text, feed_dict={text_input: [query]})\n",
    "    sim = list(cosine_similarity(sent_embedding, query_embedding).flatten())\n",
    "    score = get_label_scores(query, sent)\n",
    "    #print(score)\n",
    "    return (sim[0], score[0], score[1], score[2])\n",
    "\n",
    "def get_training_data_for_sample(sample):\n",
    "    LIMIT = 4\n",
    "    if sample['label'] == 'NOT ENOUGH INFO':\n",
    "        output = label_and_evidenve(sample['claim'] , LIMIT)\n",
    "    else:\n",
    "        docids = [get_evidence_docid(evidence) for evidence in sample['evidence'][:LIMIT]]\n",
    "        if len(docids) == 4:\n",
    "            output = [*map(lambda docid:(get_evidence_output(docid, sample['claim'])), docids)]\n",
    "        else:\n",
    "            output_a = [*map(lambda docid:(get_evidence_output(docid, sample['claim'])), docids)]\n",
    "            if len(docids) == 1:\n",
    "                output_b = [*map(lambda k:(get_evidence_output(docids[0] + k, sample['claim'])), list(range(1,4)))]\n",
    "                output = output_a + output_b \n",
    "            if len(docids) == 2:\n",
    "                output_b = [*map(lambda docid:(get_evidence_output(docid + 1, sample['claim'])), docids)]\n",
    "                output = output_a + output_b \n",
    "            if len(docids) == 3:\n",
    "                output_b = [get_evidence_output(docids[0] + 1, sample['claim'])]\n",
    "                output = output_a + output_b\n",
    "                \n",
    "    return output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Using SNLI and training data to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit mlp model on problem 2 and save model to file\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('snli_1.0_train.csv')\n",
    "df_train = df_train.drop(set(list(df_train.columns))-set(['gold_label','sentence1','sentence2']),axis=1)\n",
    "df_train=df_train.iloc[:1000]\n",
    "df_train.columns = ['label','claim','evidence']\n",
    "claim_evidence = []\n",
    "labels = []\n",
    "for index, row in df_train.iterrows():\n",
    "    labels.append((row['label']))\n",
    "    claim_evidence.append(((row['claim'],row['evidence'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_for_snli_data(hyp, premise):\n",
    "    hyp_embedding = session.run(embedded_text, feed_dict= {text_input: [hyp]})\n",
    "    premise_embedding = session.run(embedded_text, feed_dict={text_input: [premise]})\n",
    "    sim = list(cosine_similarity(hyp_embedding, premise_embedding).flatten())\n",
    "    score = get_label_scores(hyp, premise)\n",
    "    #print(score)\n",
    "    return (sim[0], score[0], score[1], score[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('snli_1.0_train.csv')\n",
    "df_train = df_train.drop(set(list(df_train.columns))-set(['gold_label','sentence1','sentence2']),axis=1)\n",
    "df_train = df_train.iloc[999:9999]\n",
    "df_train.columns = ['label','claim','evidence']\n",
    "claim_evidence_train_large = []\n",
    "labels_train_large = []\n",
    "for index, row in df_train.iterrows():\n",
    "    labels_train_large.append((row['label']))\n",
    "    claim_evidence_train_large.append(((row['claim'],row['evidence'])))\n",
    "snli_output_train_large = []\n",
    "for hyp, premise in claim_evidence_train_large:\n",
    "    snli_output_train_large.append(get_output_for_snli_data(hyp, premise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv('snli_1.0_dev.csv')\n",
    "df_dev = df_dev.drop(set(list(df_dev.columns))-set(['gold_label','sentence1','sentence2']),axis=1)\n",
    "df_dev=df_dev.iloc[:300]\n",
    "df_dev.columns = ['label','claim','evidence']\n",
    "claim_evidence_dev = []\n",
    "labels_dev = []\n",
    "for index, row in df_dev.iterrows():\n",
    "    labels_dev.append((row['label']))\n",
    "    claim_evidence_dev.append(((row['claim'],row['evidence'])))\n",
    "snli_output_dev = []\n",
    "for hyp, premise in claim_evidence_dev:\n",
    "    snli_output_dev.append(get_output_for_snli_data(hyp, premise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_large = pd.DataFrame(snli_output_train_large)# TRAINX is a list\n",
    "train_y_large = pd.DataFrame(labels_train_large)#TRAINY IS A LIST\n",
    "train_y_large = temp.fit_transform(train_y_large)\n",
    "train_y_large.columns = ['SUPPORTS','REFUTES','NOT_ENOUGH_INFO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading training and development datasamples into dataframes\n",
    "train_X = pd.DataFrame(snli_output_train_large)# TRAINX is a list\n",
    "train_y = pd.DataFrame(labels_train_large)#TRAINY IS A LIST\n",
    "dev_X = pd.DataFrame(snli_output_dev)# DEVX is a list\n",
    "dev_y = pd.DataFrame(labels_dev)# DEVY is a list\n",
    "#ENCODING our labels as one hot encoded\n",
    "temp = ce.OneHotEncoder()\n",
    "train_y = temp.fit_transform(train_y)\n",
    "train_y.columns = ['REFUTES','SUPPORTS','NOT_ENOUGH_INFO']#WARNING: naming is done by looking at dataframes manually to be consistent!!!!\n",
    "dev_y = temp.fit_transform(dev_y)\n",
    "dev_y.columns = ['NOT_ENOUGH_INFO','REFUTES','SUPPORTS']#WARNING: naming is done by looking at dataframes manually to be consistent!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform training data\n",
    "\n",
    "%%time\n",
    "failure = []\n",
    "for sample in train_sample:\n",
    "    try:\n",
    "        training_data.append(get_training_data_for_sample(sample))\n",
    "    except:\n",
    "        failure.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1172 in training_sample has problrm\n",
    "training_data_labels_2000 = []\n",
    "for sample in train_sample[:1172]:\n",
    "    if sample['label'] == 'NOT ENOUGH INFO':\n",
    "        training_data_labels_2000 .extend(['NOT ENOUGH INFO']*3)\n",
    "    else:\n",
    "        evi_len = len(sample['evidence']) \n",
    "        if evi_len >= 3:\n",
    "            training_data_labels_2000 .extend([sample['label']]*3)\n",
    "        else:\n",
    "            training_data_labels_2000 .extend([sample['label']]*evi_len)\n",
    "            training_data_labels_2000 .extend(['NOT ENOUGH INFO']*(3 - evi_len))\n",
    "for sample in train_sample[1173:2000]:\n",
    "    if sample['label'] == 'NOT ENOUGH INFO':\n",
    "        training_data_labels_2000 .extend(['NOT ENOUGH INFO']*3)\n",
    "    else:\n",
    "        evi_len = len(sample['evidence']) \n",
    "        if evi_len >= 3:\n",
    "            training_data_labels_2000 .extend([sample['label']]*3)\n",
    "        else:\n",
    "            training_data_labels_2000 .extend([sample['label']]*evi_len)\n",
    "            training_data_labels_2000 .extend(['NOT ENOUGH INFO']*(3 - evi_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loading training and development datasamples into dataframes\n",
    "train_X = pd.DataFrame(train_data)# TRAINX is a list\n",
    "train_y = pd.DataFrame(training_data_labels_2000)#TRAINY IS A LIST\n",
    "dev_X = pd.DataFrame(snli_output_dev)# DEVX is a list\n",
    "dev_y = pd.DataFrame(labels_dev)# DEVY is a list\n",
    "#ENCODING our labels as one hot encoded\n",
    "temp = ce.OneHotEncoder()\n",
    "train_y = temp.fit_transform(train_y)\n",
    "train_y.columns = ['NOT_ENOUGH_INFO','REFUTES','SUPPORTS']#WARNING: naming is done by looking at dataframes manually to be consistent!!!!\n",
    "dev_y = temp.fit_transform(dev_y)\n",
    "dev_y.columns = ['NOT_ENOUGH_INFO','SUPPORTS', 'REFUTES']#WARNING: naming is done by looking at dataframes manually to be consistent!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Fit and re-fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://machinelearningmastery.com/\n",
    "\n",
    "# define and fit model on a training dataset\n",
    "def fit_model(trainX, trainy, devX, devy):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(5, input_dim = 4, activation='relu',use_bias=True, kernel_initializer='he_uniform'))#Hidden layer with 5 units\n",
    "    #'he_uniform' draws inital weights from uniform distribution\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit model\n",
    "    history = model.fit(trainX, trainy, validation_data=(devX, devy), epochs=100, verbose=0,batch_size=128)\n",
    "    return model, history\n",
    "\n",
    "# summarize the performance of the fit model\n",
    "def summarize_model(model, history, trainX, trainy, devX, devy):\n",
    "    # evaluate the model\n",
    "    _, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "    _, dev_acc = model.evaluate(devX, devy, verbose=0)\n",
    "    print('Train: %.3f, Test: %.3f' % (train_acc, dev_acc))\n",
    "    # plot loss during training\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title('Loss')\n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.plot(history.history['val_loss'], label='dev')\n",
    "    pyplot.legend()\n",
    "    # plot accuracy during training\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title('Accuracy')\n",
    "    pyplot.plot(history.history['acc'], label='train')\n",
    "    pyplot.plot(history.history['val_acc'], label='dev')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model on train dataset\n",
    "model, history = fit_model(train_X, train_y, dev_X, dev_y)\n",
    "# evaluate model behavior\n",
    "summarize_model(model, history, trainX, trainy, devX, devy)\n",
    "#!python3 -m pip install h5py --user\n",
    "model.save('model_snli.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "# define and fit model on a training dataset\n",
    "def refit_model(trainX, trainy, devX, devy):\n",
    "    # load the model we had stored earlier\n",
    "    # model trainded by 2000 claims in the traing data\n",
    "    model = load_model('model_snli_large2.h5')\n",
    "\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit model\n",
    "    history = model.fit(trainX, trainy, validation_data=(devX, devy), epochs=100, verbose=0,batch_size=128)\n",
    "    return model, history\n",
    "\n",
    "# summarize the performance of the fit model\n",
    "def summarize_model(model, history, trainX, trainy, devX, devy):\n",
    "    # evaluate the model\n",
    "    _, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "    _, dev_acc = model.evaluate(devX, devy, verbose=0)\n",
    "    print('Train: %.3f, Test: %.3f' % (train_acc, dev_acc))\n",
    "    # plot loss during training\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title('Loss')\n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.plot(history.history['val_loss'], label='dev')\n",
    "    pyplot.legend()\n",
    "    # plot accuracy during training\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title('Accuracy')\n",
    "    pyplot.plot(history.history['acc'], label='train')\n",
    "    pyplot.plot(history.history['val_acc'], label='dev')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "  \n",
    "  \n",
    "# refit model on train dataset\n",
    "robust_model, history = refit_model(train_X, train_y, dev_x_0to500, dev_y_0to500)\n",
    "# evaluate model behavior\n",
    "summarize_model(robust_model, history, train_X, train_y, dev_x_0to500, dev_y_0to500)\n",
    "#!python3 -m pip install h5py --user\n",
    "model_train_0to2000.h5\n",
    "#robust_model.save('model_train_0to2000.h5')#OUR FINAL MODEL FOR PREDICTING TEST DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0527 01:26:24.458389 140736874374080 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0527 01:47:04.355772 140736874374080 resolver.py:330] Deleting lock file /var/folders/nm/11p2hm4d6mq71ld5bn3cmpjh0000gn/T/tfhub_modules/1fb57c3ffe1a38479233ee9853ddd7a8ac8a8c47.lock due to inactivity.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/linzhijia/Library/Python/3.6/lib/python/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0527 01:56:13.592516 140736874374080 deprecation.py:323] From /Users/linzhijia/Library/Python/3.6/lib/python/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0527 01:56:15.254437 140736874374080 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# Create graph and finalize (finalizing optional but recommended).\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "    sent_embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")\n",
    "    embedded_text = sent_embed(text_input)\n",
    "    init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    g.finalize()\n",
    "\n",
    "# Create session and initialize.\n",
    "session = tf.Session(graph=g)\n",
    "session.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using entaiment predicttor to get three probablities\n",
    "def get_label_scores(query, sentence):\n",
    "    return predictor.predict(hypothesis = query,premise = sentence)['label_probs']\n",
    "\n",
    "def get_label(scores):\n",
    "    sup, ref, noI = scores\n",
    "    if max(sup, ref, noI) == sup:\n",
    "        return \"SUPPORTS\"\n",
    "    if max(sup, ref, noI) == ref:\n",
    "        return \"REFUTES\"\n",
    "    if max(sup, ref, noI) == noI:\n",
    "        return \"NOT ENOUGH INFO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronouns_one = [' he ', 'He ', 'She ', ' she ', 'It ', ' it ']\n",
    "pronouns_two = [' his ', 'His ', ' her ', 'Her ', ' its ', 'Its ']\n",
    "\n",
    "def sort(tup): \n",
    "    return(sorted(tup, key = lambda x: float(x[0]), reverse = True))\n",
    "\n",
    "def clean_sent(sent):\n",
    "    return re.sub('\\'', '', re.sub( \" -RRB-\",')', re.sub(\"-LRB- \",'(',re.sub(\" $\" , '', re.sub('.\\n','', sent)))))\n",
    "\n",
    "scores_list = []\n",
    "def label_and_evidenve(query, k):\n",
    "\n",
    "    result = find_possible_sentences(query, 5)\n",
    "    \n",
    "    messages = []\n",
    "    ids = []\n",
    "    \n",
    "    \n",
    "    for docid,pair in result:\n",
    "        norm_id = unicodedata.normalize('NFD',re.sub('_',' ', re.sub('-LRB-(\\w)+-RRB-','', pair[0])))\n",
    "        sent = clean_sent(all_wiki_sentences[docid])\n",
    "        for word in pronouns_one:\n",
    "            if word in sent:\n",
    "                sent = re.sub(word, ' '+ norm_id + ' ', sent)\n",
    "        for word in pronouns_two:\n",
    "            if word in sent:\n",
    "                sent = re.sub(word, ' ' + norm_id + '\\'s ', sent)    \n",
    "        messages.append(re.sub('.\\n','',sent))\n",
    "        ids.append(identifier[docid])\n",
    "\n",
    "    \n",
    "    sent_embeddings = [*map(lambda sent:(session.run(embedded_text, feed_dict={text_input: [sent]})), messages)]\n",
    "    query_embedding = session.run(embedded_text, feed_dict={text_input: [query]})\n",
    "\n",
    "    # Sentences re-ranking using embedding\n",
    "    similarities = [*map(lambda sent_embedding:(cosine_similarity(sent_embedding, query_embedding).flatten()), sent_embeddings)]\n",
    "    sorted_pair = sort(list(zip([list(i)[0] for i in similarities], list(range(len(messages))))))\n",
    "    \n",
    "    #evidence = []\n",
    "    unlabeled_sents_ids = []\n",
    "    sim = []\n",
    "    unlabeled_sents = []\n",
    "    output = []\n",
    "    for score,index in sorted_pair[:k]:\n",
    "        #evidence.append(sent_embeddings[index])\n",
    "        unlabeled_sents_ids.append(ids[index])\n",
    "        sim.append(score)\n",
    "        unlabeled_sents.append(messages[index])\n",
    "        \n",
    "    scores = [*map(lambda sent:(get_label_scores(query, sent)), unlabeled_sents)]\n",
    "\n",
    "    output = [] \n",
    "    i = 0\n",
    "    for sup, ref, noI in scores:\n",
    "        output.append((sim[i], sup, ref, noI), (unlabeled_sents_ids[i]))\n",
    "        i += 1\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('test-unlabelled.json') as f:\n",
    "    test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_sents= dict()\n",
    "for i in devset:\n",
    "    query = test[i]['claim']\n",
    "    output = label_and_evidenve(query, 5)\n",
    "    test[query] = output\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = defaultdict(list)\n",
    "for i in test_sents:\n",
    "    results = []\n",
    "    ids = []\n",
    "    for sents in test_sents[i]:\n",
    "        for sent in sents:\n",
    "            ids.append(sent[1])\n",
    "            result = list(model.predict(pd.DataFrame(list(sent[0])).T).flatten())\n",
    "            results.append(result)\n",
    "    predictions[i] = [results, ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/uclmr/fever/blob/master/jack_reader.py\n",
    "def aggregate_preds(predictions, top_one_sent=False):\n",
    "    \"\"\"return the most popular label\n",
    "    \"\"\"\n",
    "    vote = dict()\n",
    "    preds = []\n",
    "    for pred in predictions[0]:\n",
    "        preds.append([\"SUPPORTS\", \"NOT ENOUGH INFO\", \"REFUTES\"][np.argmax(pred)])\n",
    "\n",
    "    for pred in preds:\n",
    "        if pred not in vote:\n",
    "            vote[pred] = 1\n",
    "        else:\n",
    "            vote[pred] += 1\n",
    "    \n",
    "    supports = \"SUPPORTS\"\n",
    "    refutes = \"REFUTES\"\n",
    "    nei = \"NOT ENOUGH INFO\"\n",
    "    \n",
    "    # believe more-likely evidence if both supports and refutes appears in the pred_list\n",
    "    if supports in vote and refutes in vote:\n",
    "        for pred in preds:\n",
    "            if pred in [supports, refutes]:\n",
    "                final_verdict = pred\n",
    "                break\n",
    "    elif supports in vote:\n",
    "        final_verdict = supports\n",
    "    elif refutes in vote:\n",
    "        final_verdict = refutes\n",
    "    else:\n",
    "        final_verdict = nei\n",
    "    if top_one_sent:\n",
    "        final_verdict = preds[0]\n",
    "\n",
    "     \n",
    "\n",
    "    if final_verdict != nei:\n",
    "        for temp in predictions[1]:\n",
    "            page_id, sent_num = temp\n",
    "            try:\n",
    "                evidence = (page_id, int(sent_num))\n",
    "                evidences.append(evidence)\n",
    "            except:\n",
    "                    # Some values of sent_nums are string and could not be \n",
    "                    # transfer to integer, we will store the original value.\n",
    "                print(\"Obammmmmaaa\")\n",
    "                evidences.append(temp)\n",
    "                continue\n",
    "\n",
    "    return (final_verdict, evidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_labels_and_evidences = dict()\n",
    "for claim in predictions:\n",
    "    temp = predictions[claim]\n",
    "    final_labels_and_evidences[claim] = aggregate_preds(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test:\n",
    "    if test[i][\"claim\"] in final_labels_and_evidences.keys():\n",
    "        test[i][\"label\"] = final_labels_and_evidences[test[i][\"claim\"]][0]\n",
    "        test[i][\"evidence\"] = final_labels_and_evidences[test[i][\"claim\"]][1]\n",
    "    else:\n",
    "        test[i][\"label\"] = \"NOT ENOUGH INFO\"\n",
    "        test[i][\"evidence\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testOutput.json', 'w') as fp:\n",
    "    json.dump(test, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Parellized entaiment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "@ray.remote\n",
    "def get_label_probs2(h,p):\n",
    "    return predictor.predict(h,p)['label_probs']\n",
    "def get_label_probs1(hypothesis,premises):\n",
    "    ray.init(num_cpus=2,num_gpus=1,ignore_reinit_error=True)\n",
    "    temp_results = ray.get([get_label_probs2.remote(hypothesis,premise) for premise in premises])\n",
    "    return temp_results\n",
    "#Chika's processed test sentences\n",
    "test_processed = None\n",
    "hypothesis_evidences = {}\n",
    "with open(\"test_sents.pkl\",'rb') as f:\n",
    "    test_processed = pickle.load(f)\n",
    "for key,value in test_processed.items():\n",
    "    temp = []\n",
    "    #print(key)\n",
    "    for tuple_3 in value:\n",
    "        temp.append(tuple_3[0])\n",
    "        hypothesis_evidences[key]=temp\n",
    "    #print(\"----------------------------------\")\n",
    "    \n",
    "    \n",
    "st=time.time()\n",
    "final_results = []\n",
    "for hypothesis,evidences in hypothesis_evidences.items():\n",
    "    final_results.append(get_label_probs1(hypothesis,evidences))\n",
    "print((time.time()-st)/60,\"minutes\")\n",
    "#TO store {claim:allennlpscores}\n",
    "claim_allennlpscores_900 = {}\n",
    "for index,hyp_evis in enumerate(hypothesis_evidences.items()):\n",
    "    #print(len(hyp_evis[1])==len(final_results[index]))\n",
    "    key = hyp_evis[0]#THE CLAIM\n",
    "    value = final_results[index]\n",
    "    claim_allennlpscores[key] = value\n",
    "with open('claim_allennlpscores.pkl', 'wb') as output:\n",
    "    pickle.dump(claim_allennlpscores, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
